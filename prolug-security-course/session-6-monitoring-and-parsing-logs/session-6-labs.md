---
cover: ../../.gitbook/assets/lab-ezgif.png
coverY: -223.29856057849736
layout:
  width: default
  cover:
    visible: true
    size: full
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
  metadata:
    visible: true
---

# üõ†Ô∏è Session 6 Labs

I'm building a comprehensive enterprise logging infrastructure through progressively complex architectures.

<details>

<summary>üèóÔ∏è What We'll Build</summary>

A **production-grade logging and monitoring ecosystem** with multiple aggregation architectures:

* [ ] Traditional rsyslog forwarding with centralized collection and filtering
* [ ] Modern agent-based architecture using Grafana Loki and Promtail
* [ ] Enterprise message queue implementation with Apache Kafka integration
* [ ] Real-time log analysis and visualization with Grafana dashboards
* [ ] Scalable, resilient logging patterns for distributed systems

</details>

### ‚è≥ Lab Progression

{% stepper %}
{% step %}
**üì° Traditional Collection ‚Üí Deploy rsyslog forwarding infrastructure**
{% endstep %}

{% step %}
**üîç Modern Observability ‚Üí Implement agent-based Loki architecture**
{% endstep %}

{% step %}
**üöÄ Event-Driven Systems ‚Üí Integrate Kafka message queues**
{% endstep %}

{% step %}
**‚òÅÔ∏è Cloud-Native (Optional) ‚Üí Explore AWS VPC Flow Logs**
{% endstep %}
{% endstepper %}

{% hint style="info" %}
**Real-World Context**

In enterprise environments, logging isn't a single solution‚Äîit's an ecosystem. You'll encounter:

* **Legacy systems** running rsyslog (the 80% of infrastructure that "just works")
* **Modern observability stacks** like Loki, Prometheus, and Grafana
* **Event-driven architectures** using Kafka, RabbitMQ, or cloud-native services

This lab exposes you to all three patterns so you understand the trade-offs and can architect appropriate solutions.
{% endhint %}

This lab teaches you the complete logging architecture landscape so you can design systems that balance real-time analysis, scalability, reliability, compliance, and cost.

By the end, we'll understand:

* The **architectural patterns** for log aggregation (traditional, agent-based, message queue, cloud-native)
* The **technical implementation** of each pattern (configuration, data flow, security considerations)
* The **operational characteristics** (scalability, latency, reliability, complexity)
* The **enterprise tradeoffs** (why you'd choose one pattern over another)

And have hands-on experience with production logging architectures that power real-world infrastructure at scale.

***

### üìã Required Materials

{% tabs %}
{% tab title="üõ†Ô∏è Tools" %}
**SSH Client**:

* PuTTY (Windows)
* OpenSSH (Linux/macOS)
* Terminal emulator with SSH support

**Lab Environment**:

* ProLUG Lab Server access (Hammer server)
* Root or sudo privileges
* Access to KillerCoda interactive lab environment

**Browser**:

* Modern web browser for Grafana UI access
* JavaScript enabled for interactive dashboards
{% endtab %}

{% tab title="üìö Prerequisites" %}
<table data-full-width="true"><thead><tr><th width="375" valign="top">Required Knowledge</th><th valign="top">Required Skills from Previous Sessions</th></tr></thead><tbody><tr><td valign="top"><ul><li>Systemd service management and journald</li><li>Basic networking concepts (TCP/IP, ports, protocols)</li><li>HTTP/REST API fundamentals</li><li>JSON data format and parsing</li><li>Configuration file editing and syntax</li><li>Command-line tools (<code>curl</code>, <code>jq</code>, <code>grep</code>)</li></ul></td><td valign="top"><ul><li>SSH access and remote system management</li><li>Log file location and analysis (<code>/var/log/*</code>)</li><li>Service debugging (<code>systemctl status</code>, log inspection)</li><li>Basic scripting (bash, python) for automation</li><li>Firewall concepts for service connectivity</li></ul></td></tr></tbody></table>
{% endtab %}
{% endtabs %}

***

### üõ†Ô∏è Labs to Build Here

{% stepper %}
{% step %}
#### **üì° Architecture 1: Rsyslog Forwarding and Collection**

Build traditional centralized logging with rsyslog‚Äîthe foundation of Linux system logging.

{% hint style="info" %}
**Architectural Pattern: Direct Syslog Forwarding**

**Characteristics**:

* Built into every modern Linux system (no agents required)
* Low resource overhead (lightweight, efficient)
* Reliable transport with TCP support
* Template-based filtering and routing
* Simple configuration for basic use cases

**Enterprise Use Cases**:

* Legacy infrastructure integration
* High-availability clusters with shared logging
* Compliance requirements for centralized audit logs
* Network devices and embedded systems (limited agent support)
{% endhint %}

**üèõÔ∏è Architecture Overview**

Consider this pattern where all Linux systems have built-in rsyslog capabilities:

* One system acts as the **log collector** (aggregation server)
* Multiple systems act as **log forwarders** (clients sending logs)
* Log files are **split by facility and priority** on the collector
* Templates control **where and how logs are stored**

{% hint style="warning" %}
**Security Consideration**

In this basic lab, communication is **unencrypted**. We'll revisit this in Unit 10 with TLS certificate-based authentication and encryption.

For now, focus on understanding the data flow and configuration patterns.
{% endhint %}

**üìñ Lab Instructions**

**Complete the KillerCoda Lab**

Navigate to: https://killercoda.com/het-tanis/course/Linux-Labs/206-setting-up-rsyslog

Follow the lab instructions to:

* Configure the rsyslog server (collector)
* Configure rsyslog clients (forwarders)
* Implement template-based log routing
* Verify log collection and filtering

**Analyze Configuration Patterns**

After completing the lab, answer these critical analysis questions:

<details>

<summary><strong>Why do we split out the logs in this lab? Why don't we just aggregate them to one place?</strong></summary>

**Analysis Framework**:

1. **What Do We Split By?**
   * **Facility**: System component generating the log (e.g., `authpriv`, `cron`, `mail`, `kern`)
   * **Priority/Severity**: Log importance level (e.g., `emerg`, `alert`, `crit`, `err`, `warning`, `info`, `debug`)
2.  **Why Split Instead of Aggregate Everything?**

    **Operational Benefits**:

    * **Troubleshooting Speed**: Find relevant logs faster without grepping millions of lines
    * **Access Control**: Security logs in separate files with restricted permissions
    * **Retention Policies**: Keep critical logs longer, rotate verbose logs more frequently
    * **Performance**: Smaller files = faster searches, less I/O contention

    **Compliance Requirements**:

    * **Audit Separation**: Authentication logs must be isolated for NIST 800-53, PCI-DSS
    * **Log Integrity**: Critical logs protected from accidental deletion or tampering
    * **Forensic Analysis**: Chain of custody maintained for security investigations
3.  **Real-World Example**:

    ```
    # Without splitting (all logs in one file)
    /var/log/messages:
    - auth failures
    - kernel messages  
    - cron jobs
    - mail delivery
    - application logs
    Total: 10GB/day, search takes minutes

    # With splitting (organized by facility/priority)
    /var/log/secure          -> auth logs only (50MB/day)
    /var/log/kern.log        -> kernel messages (100MB/day)
    /var/log/cron            -> scheduled tasks (20MB/day)
    /var/log/maillog         -> email system (500MB/day)
    /var/log/critical        -> emergencies only (1MB/day)

    Search for auth failure: grep /var/log/secure (50MB vs 10GB)
    ```
4.  **Template Configuration Mechanism**: Rsyslog templates define **where logs go based on facility.priority**:

    ```
    # Template syntax
    $template RemoteAuthLog,"/var/log/remote/%HOSTNAME%/auth.log"
    authpriv.*   ?RemoteAuthLog

    # Breakdown:
    # authpriv.*        = facility.priority (all auth logs)
    # ?RemoteAuthLog    = dynamic file path using template
    # %HOSTNAME%        = creates per-host directories
    ```

**Answer**: We split logs for operational efficiency, compliance requirements, and forensic capability. Single-file aggregation creates performance bottlenecks, complicates access control, and violates security frameworks.

</details>

<details>

<summary><strong>How does that template configuration work?</strong></summary>

**Template Configuration Anatomy**:

{% code title="/etc/rsyslog.conf - Template Example" lineNumbers="true" %}
```bash
# Define template
$template RemoteAuthLog,"/var/log/remote/%HOSTNAME%/auth.log"

# Use template with facility.priority selector
authpriv.*   ?RemoteAuthLog
```
{% endcode %}

**Component Breakdown**:

1. **Template Definition**: `$template <name>,<format>`
   * `RemoteAuthLog` = template name (used later in rules)
   * `"/var/log/remote/%HOSTNAME%/auth.log"` = file path pattern
2. **Dynamic Variables**:
   * `%HOSTNAME%` ‚Üí Resolves to source system's hostname
   * `%FROMHOST-IP%` ‚Üí Source IP address
   * `%PROGRAMNAME%` ‚Üí Application that generated the log
   * `%TIMESTAMP%` ‚Üí Log generation time
   * `%msg%` ‚Üí Actual log message content
3. **Selector Syntax**: `facility.priority action`
   * `authpriv.*` = Match all priorities for authpriv facility
   * `*.err` = Match error-level and above for all facilities
   * `mail.info` = Match info-level and above for mail facility
4. **Action Types**:
   * `?TemplateName` ‚Üí Write to dynamic file path using template
   * `@@remote-server:514` ‚Üí Forward to remote server via TCP
   * `@remote-server:514` ‚Üí Forward via UDP (less reliable)
   * `-/var/log/file` ‚Üí Async write (buffered, better performance)

**Real-World Template Examples**:

{% code title="Per-host and Date-based Templates" lineNumbers="true" %}
```bash
# Per-host directory structure
$template PerHostLog,"/var/log/remote/%HOSTNAME%/%PROGRAMNAME%.log"
*.*   ?PerHostLog

# Date-based rotation
$template DailyLog,"/var/log/remote/%HOSTNAME%/%$YEAR%-%$MONTH%-%$DAY%.log"
*.*   ?DailyLog
```
{% endcode %}

**Why Templates Matter**:

* **Scalability**: One collector can organize logs from thousands of systems
* **Automation**: Logs automatically sorted without manual intervention
* **Forensics**: Find specific host/application logs instantly
* **Compliance**: Audit trail organized by system and date

</details>

<details>

<summary><strong>Are we securing this communication in any way, or do we still need to configure that?</strong></summary>

**Current Security Posture**: ‚ùå **UNENCRYPTED**

In the basic lab configuration:

* Logs transmitted in **plaintext** over the network
* No **authentication** of sender or receiver
* No **integrity verification** (logs could be modified in transit)
* Vulnerable to **eavesdropping** and **man-in-the-middle attacks**

**What This Means**:

```
# Current configuration (INSECURE)
Client                    Network                    Server
  |                          |                         |
  |------ plaintext logs --->|<--- anyone can read     |
  |                          |                         |
```

**Security Requirements Still Needed**:

1. **Encryption**: TLS/SSL to protect logs in transit
2. **Authentication**: Certificates to verify sender/receiver identity
3. **Integrity**: Checksums to detect tampering
4. **Access Control**: Firewall rules, SELinux policies

**What We'll Add in Unit 10**:

{% code title="/etc/rsyslog.conf - TLS Configuration" lineNumbers="true" %}
```bash
# Load TLS modules
$ModLoad imtcp
$DefaultNetstreamDriver gtls

# Certificate paths
$DefaultNetstreamDriverCAFile /etc/pki/rsyslog/ca.pem
$DefaultNetstreamDriverCertFile /etc/pki/rsyslog/cert.pem
$DefaultNetstreamDriverKeyFile /etc/pki/rsyslog/key.pem

# Enforce certificate authentication
$ActionSendStreamDriverAuthMode x509/name
$ActionSendStreamDriverPermittedPeer *.example.com
$ActionSendStreamDriverMode 1 # TLS only

# Encrypted forwarding
*.* @@(o)logserver.example.com:6514
```
{% endcode %}

**Security Layers We'll Implement**:

| Layer                 | Technology                | Purpose               |
| --------------------- | ------------------------- | --------------------- |
| Transport Encryption  | TLS 1.2+                  | Prevent eavesdropping |
| Mutual Authentication | X.509 Certificates        | Verify identity       |
| Message Integrity     | HMAC                      | Detect tampering      |
| Network Segmentation  | Firewall rules            | Restrict access       |
| Access Control        | SELinux, File permissions | Protect log files     |

**Answer**: No, we are NOT securing this communication. The current lab uses plaintext transmission for learning purposes. In Unit 10, we'll implement TLS encryption, certificate-based authentication, and comprehensive security controls required for production environments.

</details>

{% hint style="danger" %}
**Production Warning**

Never deploy unencrypted log forwarding in production environments containing:

* Authentication logs (passwords, session IDs)
* Financial transactions
* Personal identifying information (PII)
* Health records (HIPAA)
* Credit card data (PCI-DSS)

Unencrypted logging violates most compliance frameworks.
{% endhint %}

{% hint style="success" %}
‚úÖ **Outcome**:

* Understanding of rsyslog architecture and configuration
* Experience with template-based log routing and filtering
* Knowledge of facility/priority-based log organization
* Awareness of security requirements for production logging
* Foundation for implementing TLS-secured rsyslog in Unit 10
{% endhint %}
{% endstep %}

{% step %}
#### **üîç Architecture 2: Agent-Based Centralized Platform (Grafana Loki)**

Implement modern observability with Grafana Loki‚Äîcloud-native log aggregation inspired by Prometheus.

{% hint style="info" %}
**Architectural Pattern: Agent-Based Log Aggregation**

**Characteristics**:

* **Agents** (Promtail) collect and push logs to centralized platform
* **Loki** stores logs with labels (metadata) for efficient querying
* **Grafana** provides visualization and dashboarding
* Designed for **cloud-native**, **containerized** environments
* Scales horizontally with distributed architecture

**Enterprise Use Cases**:

* Kubernetes cluster logging
* Microservices observability
* Multi-cloud log aggregation
* Real-time monitoring and alerting
* Integration with Prometheus metrics
{% endhint %}

**üèõÔ∏è Architecture Overview**

Review the Loki architecture: https://grafana.com/docs/loki/latest/get-started/architecture/

**Key Components**:

* **Promtail**: Agent running on each system, tailing log files and forwarding to Loki
* **Loki**: Log aggregation server that indexes logs with labels (NOT full-text indexing like Elasticsearch)
* **Grafana**: Visualization platform for querying and displaying logs
* **LogQL**: Query language for filtering and analyzing logs (similar to PromQL for metrics)

**Data Flow**: Application Logs ‚Üí Promtail (Agent) ‚Üí Loki (Storage) ‚Üí Grafana (Visualization)

**üìñ Lab Instructions**

{% stepper %}
{% step %}
**Complete the KillerCoda Lab**

Navigate to: https://killercoda.com/het-tanis/course/Linux-Labs/102-monitoring-linux-logs

Follow the lab to:

* Deploy Loki, Promtail, and Grafana
* Configure Promtail to scrape log files
* Explore Grafana for log visualization
* Understand LogQL query syntax
{% endstep %}

{% step %}
**Verify Data Flow**

After completing the lab, validate the complete pipeline:

{% code title="Verify Loki is Receiving Logs" %}
```bash
# Check Loki API health
curl http://localhost:3100/ready

# Query logs via HTTP API
curl -G -s "http://localhost:3100/loki/api/v1/query_range" \
  --data-urlencode 'query={job="varlogs"}' \
  --data-urlencode 'limit=10' | jq
```
{% endcode %}

**Expected Output**: JSON response containing log entries with labels and timestamps

{% hint style="success" %}
‚úÖ **Success Indicator**: If you see log entries in the JSON response, Loki is successfully receiving and indexing logs from Promtail.
{% endhint %}
{% endstep %}

{% step %}
**Custom Log Injection Exercise**

**Objective**: Generate custom logs and verify they appear in Grafana.

{% code title="Generate Custom Logs" overflow="wrap" lineNumbers="true" %}
```bash
# Navigate to lab directory
cd /answers

# Run custom log generator (multiple times)
python3 loki-write.py
python3 loki-write.py
python3 loki-write.py
```
{% endcode %}

**Verify in Grafana**:

1. Refresh your Grafana dashboard
2. Change the app filter to: `app="lab_logging"`
3. Confirm your custom log entries appear

<details>

<summary><strong>Can you see your logs in Grafana?</strong></summary>

If YES:

* ‚úÖ Promtail ‚Üí Loki ‚Üí Grafana pipeline is functioning
* ‚úÖ Label-based filtering is working (`app="lab_logging"`)
* ‚úÖ You understand the data flow

If NO:

* Check Promtail status: `systemctl status promtail`
* Verify Promtail configuration: `/etc/promtail/config.yml`
* Check Loki logs: `journalctl -u loki -n 50`
* Confirm network connectivity between components

</details>
{% endstep %}

{% step %}
**Modify Log Generator**

**Challenge**: Personalize the log generator to include your name.

{% code title="Edit loki-write.py" %}
```bash
# Open the Python script
vi /answers/loki-write.py

# Find the log message generation section

# Modify to include your name:

# OLD: logger.warning("Test warning from script")

# NEW: logger.warning("Test warning from YourName's script")
```
{% endcode %}

**Run Modified Script**:

```bash
python3 loki-write.py
```

**Verify in Grafana**:

* Search for logs containing your name
* Confirm your personalized logs are being ingested

{% hint style="info" %}
**Learning Point**: This demonstrates how application-generated logs (not just system logs) flow through the Loki pipeline. In production, your applications would use similar logging libraries to send structured logs to Loki.
{% endhint %}
{% endstep %}

{% step %}
**Query Logs via HTTP API**

**Objective**: Understand how to programmatically query Loki without Grafana UI.

{% code title="Rate Query" overflow="wrap" %}
```bash
# Query log rate over time
curl -G -s "http://localhost:3100/loki/api/v1/query_range" \
  --data-urlencode 'query=sum(rate({job="lab_logging"}[10m])) by (level)' \
  --data-urlencode 'step=300' | jq
```
{% endcode %}

**What This Query Does**:

* `rate({job="lab_logging"}[10m])` ‚Üí Calculate log rate over 10-minute window
* `sum(...) by (level)` ‚Üí Aggregate by log level (INFO, WARNING, ERROR)
* `step=300` ‚Üí Return data points every 5 minutes

<details>

<summary><strong>Can you modify that to see the actual entries?</strong></summary>

**Goal**: Instead of rates/metrics, retrieve the actual log messages.

**Modified Query** (Instant Query for Recent Logs):

{% code title="Retrieve Actual Log Entries" overflow="wrap" %}
```bash
curl -G -s "http://localhost:3100/loki/api/v1/query_range" \
  --data-urlencode 'query={job="lab_logging"}' \
  --data-urlencode 'limit=50' | jq '.data.result[].values[][1]'
```
{% endcode %}

**Query Breakdown**:

* `{job="lab_logging"}` ‚Üí Select all logs with this label
* `limit=50` ‚Üí Return up to 50 log entries
* `jq '.data.result[].values[][1]'` ‚Üí Extract just the log messages (not timestamps)

**Alternative: Query with Time Range**:

{% code overflow="wrap" %}
```bash
# Get logs from last 5 minutes
curl -G -s "http://localhost:3100/loki/api/v1/query_range" \
  --data-urlencode 'query={job="lab_logging"}' \
  --data-urlencode "start=$(date -d '5 minutes ago' +%s)000000000" \
  --data-urlencode "end=$(date +%s)000000000" | jq
```
{% endcode %}

**Reference Documentation**: https://grafana.com/docs/loki/latest/reference/loki-http-api/#query-logs-within-a-range-of-time

**What You're Learning**:

* Loki exposes a full HTTP API (not just Grafana UI)
* You can query logs programmatically from scripts
* LogQL syntax for filtering and aggregation
* Time-range queries for incident investigation

</details>
{% endstep %}
{% endstepper %}

{% hint style="warning" %}
**Security Reminder**

Like rsyslog, this basic Loki deployment is **unencrypted**. We'll add TLS authentication in Unit 10. Never deploy without encryption in production environments containing sensitive logs.
{% endhint %}

{% hint style="success" %}
‚úÖ **Outcome**:

* Understanding of agent-based log architecture
* Hands-on experience with Loki, Promtail, and Grafana
* Knowledge of LogQL query language
* Ability to programmatically query logs via HTTP API
* Appreciation for cloud-native observability patterns
{% endhint %}
{% endstep %}

{% step %}
#### **üöÄ Architecture 3: Message Queues (Event Bus) for Log Aggregation**

Integrate Apache Kafka for resilient, scalable log aggregation with buffering and guaranteed delivery.

{% hint style="info" %}
**Architectural Pattern: Message Queue / Event Bus**

**Characteristics**:

* **Decouples producers and consumers** (systems can send logs even if aggregator is down)
* **Buffering and backpressure handling** (prevents log loss during outages)
* **Multiple consumers** can read from same log stream
* **Ordered, durable storage** with configurable retention
* **Horizontal scalability** with partitioning

**Enterprise Use Cases**:

* Remote locations with unreliable connectivity
* High-volume log sources that overwhelm centralized collectors
* Real-time log processing pipelines (fraud detection, security monitoring)
* Multi-destination log routing (SIEM, data lake, analytics platform)
* Event-driven architectures requiring guaranteed delivery
{% endhint %}

**üèõÔ∏è Why Apache Kafka?**

Apache Kafka is found in **80% of Fortune 100 companies**. Read about its use cases: https://kafka.apache.org/uses

**Key Capabilities**:

* **Messaging**: Publish/subscribe and queue patterns
* **Storage**: Durable, replicated commit log
* **Stream Processing**: Real-time event processing
* **Integration**: Connect disparate systems reliably

**Log Aggregation Pattern with Kafka**:

Log Sources ‚Üí Kafka (Buffer/Queue) ‚Üí Log Consumers (Loki, SIEM, Data Lake)

**Why Use Kafka for Logging?**:

* **Reliability**: Logs buffered even if downstream consumers fail
* **Scalability**: Handle millions of logs per second
* **Flexibility**: Multiple consumers can process same logs differently
* **Decoupling**: Log producers don't care about consumer availability

{% hint style="warning" %}
**Complexity Tradeoff**

Kafka adds significant operational complexity:

* Requires ZooKeeper (or KRaft mode in newer versions)
* Distributed system with partitioning, replication, leader election
* More infrastructure to monitor and maintain

Use Kafka when reliability and scale justify the complexity‚Äînot for simple logging needs.
{% endhint %}

**üìñ Lab Instructions**

{% stepper %}
{% step %}
**Review Architecture Diagram**

Before starting the lab, understand the data flow:

```
Application Logs 
    ‚Üì
Kafka Producer (sends logs to Kafka topic)
    ‚Üì
Kafka Cluster (stores logs durably)
    ‚Üì
Kafka Consumer (Promtail reads from Kafka)
    ‚Üì
Loki (stores logs for querying)
    ‚Üì
Grafana (visualizes logs)
```

**Why This Architecture?**:

* Application can send logs to Kafka and continue (doesn't wait for Loki)
* Kafka buffers logs if Loki goes down (no log loss)
* Multiple consumers can read from Kafka (send to SIEM AND Loki)
* Remote sites can send to Kafka, then backhaul to central Loki
{% endstep %}

{% step %}
**Complete the KillerCoda Lab**

Navigate to: https://killercoda.com/het-tanis/course/Linux-Labs/108-kafka-to-loki-logging

Follow the lab to:

* Deploy Kafka cluster (with ZooKeeper)
* Configure Promtail as Kafka consumer
* Send logs to Kafka topic
* Verify logs flow through Kafka ‚Üí Promtail ‚Üí Loki ‚Üí Grafana
{% endstep %}

{% step %}
**Validate Complete Pipeline**

**Verify Each Component**:

{% code title="Check Kafka Health" %}
```bash
# List Kafka topics
kafka-topics.sh --bootstrap-server localhost:9092 --list

# Check topic details
kafka-topics.sh --bootstrap-server localhost:9092 \
  --describe --topic logs

# Consume messages directly from Kafka (test)
kafka-console-consumer.sh --bootstrap-server localhost:9092 \
  --topic logs --from-beginning --max-messages 10
```
{% endcode %}

{% code title="Verify Promtail is Consuming from Kafka" %}
```bash
# Check Promtail logs for Kafka consumer activity
journalctl -u promtail -n 50 | grep -i kafka

# Expected: Messages about Kafka consumer group, offset commits
```
{% endcode %}

{% code title="Confirm Logs in Grafana" %}
```bash
# Query Loki API for logs from Kafka pipeline
curl -G -s "http://localhost:3100/loki/api/v1/query_range" \
  --data-urlencode 'query={job="kafka-logs"}' \
  --data-urlencode 'limit=10' | jq
```
{% endcode %}

{% hint style="success" %}
‚úÖ **Success Indicators**:

* Kafka topic contains messages
* Promtail logs show successful Kafka consumption
* Grafana displays logs from Kafka pipeline
* End-to-end latency is reasonable (<5 seconds)
{% endhint %}
{% endstep %}

{% step %}
**Analyze Flow and Design Decisions**

<details>

<summary><strong>Did you get it all to work? Does the flow make sense in the context of the diagram?</strong></summary>

**Validation Questions**:

1.  **Can you trace a log message through each component?**

    ```
    Application ‚Üí Kafka Topic ‚Üí Promtail Consumer ‚Üí Loki Storage ‚Üí Grafana Query
    ```
2. **What happens if Loki goes down temporarily?**
   * Logs remain in Kafka (durable storage)
   * Promtail resumes consuming when Loki recovers
   * No log loss (Kafka acts as buffer)
3. **What happens if Kafka goes down temporarily?**
   * Applications lose ability to send logs (backpressure)
   * Need application-side buffering or retry logic
   * When Kafka recovers, applications resume sending
4. **How does this differ from direct rsyslog forwarding?**

| Architecture   | Durability           | Decoupling                  | Scalability | Complexity |
| -------------- | -------------------- | --------------------------- | ----------- | ---------- |
| Direct rsyslog | Low (no buffer)      | Tight (client ‚Üí server)     | Limited     | Low        |
| Kafka + Loki   | High (durable queue) | Loose (producer ‚Üî consumer) | Very High   | High       |

**Real-World Scenario**:

You have remote retail stores sending logs to corporate data center:

* **Without Kafka**: If data center link goes down, logs are lost
* **With Kafka**: Kafka at store buffers logs, sends when link recovers

</details>
{% endstep %}

{% step %}
**Industry Research**

**Objective**: Find real-world examples of Kafka for logging.

<details>

<summary><strong>Can you find any configurations or blogs that describe why you might want to use this architecture or how it has been used in the industry?</strong></summary>

**Research Directions**:

1. **LinkedIn's Kafka Adoption**:
   * Search: "LinkedIn Kafka logging architecture"
   * They use Kafka to collect logs from thousands of microservices
   * Kafka topics per service, multiple consumers for different purposes
2. **Netflix's Real-Time Log Processing**:
   * Search: "Netflix Kafka log aggregation"
   * Kafka buffers logs from global infrastructure
   * Real-time analytics for fraud detection and recommendations
3. **Uber's Distributed Logging**:
   * Search: "Uber Kafka logging platform"
   * City-scale event processing with Kafka
   * Logs from drivers, riders, payment systems all flow through Kafka
4. **Common Patterns You'll Find**:
   * **Log shippers** (Filebeat, Fluentd) ‚Üí Kafka ‚Üí **Log storage** (Elasticsearch, S3)
   * **Application logs** ‚Üí Kafka ‚Üí **Stream processing** (Flink, Spark) ‚Üí **Alerting**
   * **Cloud events** (AWS CloudTrail) ‚Üí Kafka ‚Üí **SIEM** (Splunk, Elastic)

**Key Takeaway**: Kafka is used when **reliability and scale** are more important than **simplicity**. If you're processing millions of logs per second across distributed systems, Kafka's complexity is justified.

</details>
{% endstep %}
{% endstepper %}

{% hint style="success" %}
‚úÖ **Outcome**:

* Understanding of event-driven log architectures
* Hands-on experience with Kafka and message queues
* Knowledge of producer/consumer patterns
* Appreciation for durability and buffering in distributed systems
* Ability to evaluate when message queue complexity is justified
{% endhint %}
{% endstep %}

{% step %}
#### **‚òÅÔ∏è Architecture 4: Cloud-Native Logging Services (Optional)**

Explore managed logging services in cloud environments with AWS VPC Flow Logs.

{% hint style="info" %}
**Architectural Pattern: Cloud-Native Managed Services**

**Characteristics**:

* **Fully managed** (no infrastructure to deploy or maintain)
* **Auto-scaling** (handles any log volume)
* **Integrated** with cloud provider ecosystem
* **Pay-per-use** pricing model
* **Compliance and security** built-in

**Enterprise Use Cases**:

* Cloud-first or hybrid-cloud architectures
* Ephemeral infrastructure (containers, serverless)
* Need for global scale without operational overhead
* Compliance requirements (HIPAA, PCI-DSS with managed logging)
{% endhint %}

**üìñ Optional Lab Instructions**

{% stepper %}
{% step %}
**Setup AWS VPC Flow Logs**

**Objective**: Configure network flow logging in AWS environment.

Navigate to: https://catalog.workshops.aws/well-architected-security/en-US/3-detection/40-vpc-flow-logs-analysis-dashboard/1-enable-vpc-flow-logs

**What You'll Learn**:

* Enabling VPC Flow Logs for network traffic visibility
* Configuring log destinations (CloudWatch Logs, S3, Kinesis Data Firehose)
* Analyzing network flows for security and troubleshooting
* Creating CloudWatch dashboards for flow log visualization

{% hint style="warning" %}
**AWS Account Required**

This lab requires an active AWS account. If you don't have one:

* Skip the hands-on portion
* Review the documentation to understand VPC Flow Logs conceptually
* Consider the analysis questions below even without implementation
{% endhint %}
{% endstep %}

{% step %}
**Analyze VPC Flow Log Fields**

**Question**: Even if not completing the lab, why might it be useful to understand the fields of a VPC flow log even if you're not setting up logging in AWS environments (but your organization does use AWS)?

<details>

<summary><strong>Analysis: Why VPC Flow Log Knowledge Matters</strong></summary>

**VPC Flow Log Format**:

```
<version> <account-id> <interface-id> <srcaddr> <dstaddr> <srcport> <dstport> 
<protocol> <packets> <bytes> <start> <end> <action> <log-status>
```

**Example Flow Log Entry**:

```
2 123456789012 eni-0123456789abcdef0 192.168.1.100 203.0.113.5 
443 49152 6 10 5000 1620000000 1620000010 ACCEPT OK
```

**Why This Matters for Linux Engineers**:

1. **Security Incident Investigation**:
   * VPC Flow Logs show network traffic to/from your Linux instances
   * You can identify unauthorized access attempts, data exfiltration, DDoS attacks
   * Even if you don't configure logging, security team will provide flow logs for your investigation
2. **Performance Troubleshooting**:
   * High packet counts to specific destinations = potential bottleneck
   * REJECT actions = firewall/security group misconfiguration
   * Understanding flow logs helps diagnose connectivity issues
3. **Compliance and Auditing**:
   * Many frameworks (PCI-DSS, HIPAA) require network logging
   * You may need to prove no unauthorized access occurred
   * Flow logs provide evidence for compliance audits
4. **Integration with On-Premises Systems**:
   * Hybrid cloud architectures need correlated logging
   * VPC Flow Logs ‚Üí CloudWatch ‚Üí Lambda ‚Üí rsyslog (on-prem SIEM)
   * Understanding flow log format enables integration
5. **Cost Optimization**:
   * Identify high-bandwidth connections
   * Detect inefficient network patterns
   * Optimize egress costs by analyzing destination traffic

**Real-World Scenario**:

Security team alerts you: "Unusual traffic from your EC2 instance to external IP."

* They provide VPC Flow Logs
* You need to correlate with application logs on Linux system
* Understanding flow log fields lets you identify the process responsible

</details>

**Reference**: https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-records-examples.html
{% endstep %}
{% endstepper %}

{% hint style="info" %}
**Cloud-Native Tradeoffs**

**Advantages**:

* No infrastructure management
* Automatic scaling
* Built-in redundancy
* Integration with cloud services

**Disadvantages**:

* Vendor lock-in
* Less control over configuration
* Potential cost surprises at scale
* Data egress costs for hybrid architectures

**When to Use**: Cloud-native logging makes sense when operational simplicity outweighs the need for custom control and when workloads are primarily cloud-based.
{% endhint %}

{% hint style="success" %}
‚úÖ **Outcome**:

* Understanding of cloud-native logging patterns
* Knowledge of VPC Flow Logs for network visibility
* Ability to analyze flow log data for security and troubleshooting
* Awareness of hybrid cloud logging integration challenges
* Appreciation for managed service tradeoffs
{% endhint %}
{% endstep %}

{% step %}
#### **üéØ Digging Deeper Challenges (Optional)**

Advanced challenges for engineers ready to extend their logging architecture knowledge.

{% hint style="info" %}
**Professional Development**

These challenges aren't required for lab completion but represent real-world scenarios you'll encounter in enterprise environments. They're opportunities to:

* Synthesize knowledge across multiple logging architectures
* Analyze industry-standard solutions and anti-patterns
* Explore the Cloud Native Computing Foundation (CNCF) logging ecosystem
* Practice architectural decision-making with enterprise constraints
{% endhint %}

**üìã Challenge Tasks**

<details>

<summary><strong>Challenge 1: Metric Data Pipelining with Event Buses (Wayfair Case Study)</strong></summary>

**Objective**: Analyze how event buses solve latency and connectivity problems at enterprise scale.

**Read**: https://get.influxdata.com/rs/972-GDU-533/images/Customer%20Case%20Study\_%20Wayfair.pdf

**Focus**: Pages 14-15 (Latency and Connectivity Solutions)

**Analysis Questions**:

1. **What was Wayfair's problem?**
   * They're dealing with **metric data** (not logs), but the architecture patterns apply
   * Dispersed systems, high data volume, latency-sensitive analytics
2. **How did they use an event bus to solve it?**
   * What component acted as the event bus?
   * How did buffering help with connectivity issues?
   * What was the latency improvement?
3. **How does this relate to the Kafka logging architecture you built?**
   * Similar pattern: Producer ‚Üí Event Bus ‚Üí Consumer
   * Decoupling benefits: Systems can fail independently
   * Buffering: Data doesn't get lost during outages
4. **Could you apply this to logging?**
   * Replace metric data with log data
   * Same event bus pattern (Kafka, RabbitMQ, AWS Kinesis)
   * Benefits: Reliability, scale, multiple consumers

**Deliverable**: Write a 2-3 paragraph analysis in the Discord discussion forum explaining how Wayfair's metric pipeline relates to log aggregation architectures.

</details>

<details>

<summary><strong>Challenge 2: AWS Logging Anti-Patterns</strong></summary>

**Objective**: Learn what NOT to do in logging architectures by studying AWS Well-Architected Framework.

**Read**: https://docs.aws.amazon.com/wellarchitected/latest/framework/sec\_detect\_investigate\_events\_app\_service\_logging.html

**Analysis Questions**:

1. **How do these anti-patterns relate to your current understanding of logging?**
   * Which anti-patterns did you encounter in the labs (even unintentionally)?
   * Are there anti-patterns you've seen in production environments?
2. **Do they show anything that you need to think about in the future of how you look at enterprise logging?**

**Common Anti-Patterns to Identify**:

| Anti-Pattern                          | Why It's Bad                                            | Correct Pattern                                           |
| ------------------------------------- | ------------------------------------------------------- | --------------------------------------------------------- |
| **No centralized logging**            | Each system logs independently; impossible to correlate | Centralized aggregation (rsyslog, Loki, CloudWatch)       |
| **Logs only in ephemeral storage**    | Container restarts = log loss                           | Persistent storage or external aggregation                |
| **No log retention policy**           | Infinite storage costs; compliance violations           | Automated retention (30 days app logs, 1 year audit logs) |
| **Logging sensitive data**            | Security risk; compliance violations                    | Redact PII, passwords, API keys before logging            |
| **No monitoring of logging pipeline** | Log collection failures go unnoticed                    | Monitor log ingestion rates, pipeline health              |

**Deliverable**: Post 3 anti-patterns you found most relevant to the lab architectures, explaining how to avoid them.

</details>

<details>

<summary><strong>Challenge 3: Explore the CNCF Observability Landscape</strong></summary>

**Objective**: Understand the cloud-native logging ecosystem and how tools integrate.

**Navigate to**: https://landscape.cncf.io/guide#observability-and-analysis--observability

**Analysis Questions**:

1. **Which of these have you used and which have you not used?**
   * Identify tools from the labs (Prometheus, Grafana, Loki)
   * Note tools you haven't encountered yet
2. **How do many of these plug into existing observability patterns (logging)?**
   * Look for **collectors** (Fluentd, Fluent Bit, Vector)
   * Look for **storage** (Loki, Elasticsearch, ClickHouse)
   * Look for **visualization** (Grafana, Kibana)
   * See how they connect: Collector ‚Üí Storage ‚Üí Visualization
3. **What is Fluentd trying to solve? How does it work?**

**Fluentd Analysis**:

**Read**: https://www.fluentd.org/architecture

**What Problem Does It Solve?**:

* **Unified logging layer**: Collect logs from many sources (files, syslog, HTTP, Kafka)
* **Flexible routing**: Send logs to multiple destinations simultaneously
* **Data transformation**: Parse, filter, enrich logs before storage
* **Plugin ecosystem**: 500+ plugins for inputs, outputs, filters

**How It Works**:

```
Input Plugins ‚Üí Buffer ‚Üí Filter Plugins ‚Üí Output Plugins
```

**Example Use Case**:

```
Docker logs ‚Üí Fluentd ‚Üí [Parse JSON, Add tags] ‚Üí {
  ‚Üí Elasticsearch (for searching)
  ‚Üí S3 (for long-term storage)
  ‚Üí Kafka (for real-time processing)
}
```

**Compare to Lab Architectures**:

* **Rsyslog**: Good for syslog, limited flexibility
* **Promtail**: Designed for Loki, limited to Loki output
* **Fluentd**: Universal collector, any input ‚Üí any output

**When to Use Fluentd**:

* Multiple log sources (apps, containers, network devices)
* Multiple destinations (SIEM, data lake, monitoring)
* Complex log transformations (parsing, enrichment, redaction)

**Deliverable**: Write a comparison: "When would I use Fluentd instead of Promtail?" (Hint: It depends on your outputs and transformations needed.)

</details>

{% hint style="success" %}
‚úÖ **Outcome**:

* Deep understanding of event-driven architectures for data pipelines
* Knowledge of logging anti-patterns and how to avoid them
* Familiarity with CNCF observability ecosystem
* Understanding of universal log collectors (Fluentd) vs. specialized agents (Promtail)
* Ability to evaluate and select appropriate logging tools for enterprise needs
{% endhint %}
{% endstep %}
{% endstepper %}

{% hint style="info" %}
üéì **Professional Development**

These labs expose you to the full spectrum of logging architectures‚Äîfrom traditional syslog to modern cloud-native observability.

> _You've implemented four distinct logging patterns, each with specific strengths, weaknesses, and enterprise use cases. This knowledge allows you to architect appropriate solutions for different organizational needs._



**Career Impact**: Add these skills to your project section on your resume:

* **"Centralized log aggregation with rsyslog, Loki, and Kafka"**
* **"Cloud-native observability with Grafana and Prometheus"**
* **"Event-driven architecture design with message queues"**
* **"AWS VPC Flow Logs analysis and security monitoring"**
{% endhint %}

***
